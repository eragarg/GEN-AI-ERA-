{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab7f483-9f2f-4447-914e-7b8350140c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch torchvision matplotlib\n",
    "\n",
    "# Download dataset\n",
    "!mkdir -p data && cd data && curl -O http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/facades.tar.gz\n",
    "!cd data && tar -xvzf facades.tar.gz\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dataset class\n",
    "class Pix2PixDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.files = os.listdir(root_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.files[idx])\n",
    "        image = Image.open(img_path)\n",
    "        w, h = image.size\n",
    "        input_image = image.crop((0, 0, w//2, h))\n",
    "        target_image = image.crop((w//2, 0, w, h))\n",
    "        if self.transform:\n",
    "            input_image = self.transform(input_image)\n",
    "            target_image = self.transform(target_image)\n",
    "        return input_image, target_image\n",
    "\n",
    "# Generator (U-Net style, simplified)\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super(UNetGenerator, self).__init__()\n",
    "\n",
    "        def block(in_c, out_c, down=True, act='relu', use_dropout=False):\n",
    "            layers = []\n",
    "            if down:\n",
    "                layers.append(nn.Conv2d(in_c, out_c, 4, 2, 1, bias=False))\n",
    "            else:\n",
    "                layers.append(nn.ConvTranspose2d(in_c, out_c, 4, 2, 1, bias=False))\n",
    "            layers.append(nn.BatchNorm2d(out_c))\n",
    "            layers.append(nn.ReLU(True) if act == 'relu' else nn.LeakyReLU(0.2, True))\n",
    "            if use_dropout:\n",
    "                layers.append(nn.Dropout(0.5))\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            block(in_channels, 64, down=True, act='leaky'),\n",
    "            block(64, 128, down=True, act='leaky'),\n",
    "            block(128, 256, down=True, act='leaky'),\n",
    "            block(256, 512, down=True, act='leaky'),\n",
    "        )\n",
    "\n",
    "        self.middle = block(512, 512, down=False, act='relu')\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            block(512, 256, down=False, act='relu'),\n",
    "            block(256, 128, down=False, act='relu'),\n",
    "            block(128, 64, down=False, act='relu'),\n",
    "            nn.ConvTranspose2d(64, out_channels, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.middle(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "dataset = Pix2PixDataset(\"data/facades/train\", transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "generator = UNetGenerator().to(device)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002)\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    for i, (input_img, target_img) in enumerate(dataloader):\n",
    "        input_img = input_img.to(device)\n",
    "        target_img = target_img.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = generator(input_img)\n",
    "        loss = criterion(output, target_img)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Step [{i}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Visualize outputs\n",
    "def show_images(inputs, outputs, targets):\n",
    "    inputs = inputs.cpu().permute(0, 2, 3, 1)\n",
    "    outputs = outputs.cpu().permute(0, 2, 3, 1)\n",
    "    targets = targets.cpu().permute(0, 2, 3, 1)\n",
    "    for i in range(inputs.shape[0]):\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "        axs[0].imshow((inputs[i] * 0.5 + 0.5).numpy())\n",
    "        axs[0].set_title(\"Input\")\n",
    "        axs[1].imshow((outputs[i] * 0.5 + 0.5).numpy())\n",
    "        axs[1].set_title(\"Generated\")\n",
    "        axs[2].imshow((targets[i] * 0.5 + 0.5).numpy())\n",
    "        axs[2].set_title(\"Target\")\n",
    "        for ax in axs:\n",
    "            ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Inference\n",
    "test_input, test_target = next(iter(dataloader))\n",
    "test_input = test_input.to(device)\n",
    "with torch.no_grad():\n",
    "    test_output = generator(test_input)\n",
    "show_images(test_input, test_output, test_target)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
